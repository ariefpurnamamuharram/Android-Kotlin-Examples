{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformers from Scratch in PyTorch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPuPKV2vUBrEivhAzMad6Pf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ariefpurnamamuharram/Android-Kotlin-Examples/blob/master/Transformers_from_Scratch_in_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformers from Stratch in PyTorch\n",
        "---\n",
        "References:\n",
        "- https://medium.com/the-dl/transformers-from-scratch-in-pytorch-8777e346ca51, with some modifications by APM"
      ],
      "metadata": {
        "id": "XgC5PYine0tY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIXjMuEbX6oU",
        "outputId": "fed1a938-b097-453f-935d-5e341dc00dd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.11.1+cu111)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.7/dist-packages (0.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n"
          ]
        }
      ],
      "source": [
        "# Install the required packages\n",
        "!pip install numpy\n",
        "!pip install torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaled Dot Product Attention\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as f\n",
        "from torch import Tensor\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ScaledDotProductAttention, self).__init__()\n",
        "  \n",
        "  def forward(self, q: Tensor, k: Tensor, v: Tensor) -> Tensor:\n",
        "    # Torch dimensions:\n",
        "    # 0: Rows\n",
        "    # 1: Columns\n",
        "    # torch.bmm: Performs a batch matrix-matrix product of matrices\n",
        "    # torch.bmm: https://pytorch.org/docs/stable/generated/torch.bmm.html\n",
        "    # torch.transpose: Retruns a tensor that is a transposed version of 'input'\n",
        "    # torch.transpose: https://pytorch.org/docs/stable/generated/torch.transpose.html\n",
        "    temp = q.bmm(k.transpose(1,2)) # why self.k is transpose(1,2) NOT (0,1)?\n",
        "    scale = q.size(-1) ** 0.5\n",
        "    softmax = f.softmax(temp / scale, dim=-1)\n",
        "    return softmax.bmm(v)"
      ],
      "metadata": {
        "id": "sI9iBVXkYKA6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-Head Attention\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "\n",
        "class AttentionHead(nn.Module):\n",
        "  def __init__(self, dim_in: int, dim_k: int, dim_v: int):\n",
        "    super(AttentionHead, self).__init__()\n",
        "    self.q = nn.Linear(dim_in, dim_k)\n",
        "    self.k = nn.Linear(dim_in, dim_v)\n",
        "    self.v = nn.Linear(dim_in, dim_v)\n",
        "    self.scaled_dot_product_att = ScaledDotProductAttention()\n",
        "  \n",
        "  def forward(self, query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n",
        "    return self.scaled_dot_product_att(self.q(query), self.k(key), self.v(value))\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, num_heads: int, dim_in: int, dim_k: int, dim_v: int):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.heads = nn.ModuleList(\n",
        "        [AttentionHead(dim_in, dim_k, dim_v) for _ in range(num_heads)]\n",
        "    )\n",
        "    self.linear = nn.Linear(num_heads * dim_v, dim_in)\n",
        "  \n",
        "  def forward(self, query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n",
        "    # torch.cat: Concatenates the give sequence of 'seq' tensors in the given dimension.\n",
        "    # torch.cat: https://pytorch.org/docs/stable/generated/torch.cat.html\n",
        "    return self.linear(\n",
        "        torch.cat([h(query, key, value) for h in self.heads], dim=-1)\n",
        "    )"
      ],
      "metadata": {
        "id": "-FoJSNz5ZQZ1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Positional Encoding\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, device: torch.device = torch.device(\"cpu\")):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "    self.device = device\n",
        "  \n",
        "  def forward(self, seq_len: int, dim_model: int) -> Tensor:\n",
        "    pos = torch.arange(seq_len, dtype=torch.float, device=self.device).reshape(1, -1, 1)\n",
        "    dim = torch.arange(dim_model, dtype=torch.float, device=self.device).reshape(1, 1, -1)\n",
        "    phase = pos / 1e4 ** (dim // dim_model)\n",
        "\n",
        "    return torch.where(dim.long() % 2 == 0, torch.sin(phase), torch.cos(phase))"
      ],
      "metadata": {
        "id": "9yPQP2ygd5ld"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer Architecture\n",
        "# --------------------------\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(FeedForward, self).__init__()\n",
        "  \n",
        "  def forward(self, dim_input: int = 512, dim_feedforward: int = 2048):\n",
        "    return nn.Sequential(\n",
        "        nn.Linear(dim_input, dim_feedforward),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(dim_feedforward, dim_input)\n",
        "    )\n",
        "\n",
        "class Residual(nn.Module):\n",
        "  def __init__(self, sublayer: nn.Module, dimension: int, dropout: float = 0.1):\n",
        "    super(Residual, self).__init__()\n",
        "    self.sublayer = sublayer\n",
        "    self.norm = nn.LayerNorm(dimension)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "  \n",
        "  def forward(self, *tensors: Tensor) -> Tensor:\n",
        "    # Assume that the \"value\" tensor is given last, so we can compute the\n",
        "    # residual. This matches the signature of 'MultiHeadAttention'.\n",
        "    return self.norm(tensors[-1] + self.dropout(self.sublayer(*tensors)))\n",
        "\n",
        "\n",
        "# Transformer Encoder\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "      dim_model: int = 512,\n",
        "      num_heads: int = 6,\n",
        "      dim_feedforward: int = 2048,\n",
        "      dropout: float = 0.1\n",
        "  ):\n",
        "    super(TransformerEncoderLayer, self).__init__()\n",
        "    dim_k = dim_v = dim_model // num_heads\n",
        "    self.attention = Residual(\n",
        "        MultiHeadAttention(num_heads, dim_model, dim_k, dim_v),\n",
        "        dimension = dim_model,\n",
        "        dropout = dropout\n",
        "    )\n",
        "    self.feed_forward = Residual(\n",
        "        FeedForward()(dim_model, dim_feedforward),\n",
        "        dimension=dim_model,\n",
        "        dropout=dropout,\n",
        "    )\n",
        "  \n",
        "  def forward(self, src: Tensor) -> Tensor:\n",
        "    src = self.attention(src, src, src)\n",
        "    return self.feed_forward(src)\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "      num_layers: int = 6,\n",
        "      dim_model: int = 512,\n",
        "      num_heads: int = 8,\n",
        "      dim_feedforward: int = 2048,\n",
        "      dropout: float = 0.1,\n",
        "  ):\n",
        "    super(TransformerEncoder, self).__init__()\n",
        "    self.layers = nn.ModuleList([\n",
        "                                 TransformerEncoderLayer(dim_model, num_heads, dim_feedforward, dropout)\n",
        "                                 for _ in range(num_layers)\n",
        "    ])\n",
        "    self.position_encoding = PositionalEncoding()\n",
        "  \n",
        "  def forward(self, src: Tensor) -> Tensor:\n",
        "    seq_len, dimension = src.size(1), src.size(2)\n",
        "    src += self.position_encoding(seq_len, dimension)\n",
        "    for layer in self.layers:\n",
        "      src = layer(src)\n",
        "    \n",
        "    return src\n",
        "\n",
        "\n",
        "# Transformer Decoder\n",
        "\n",
        "class TransformerDecoderLayer(nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "      dim_model: int = 512,\n",
        "      num_heads: int = 6, \n",
        "      dim_feedforward: int = 2048,\n",
        "      dropout: float = 0.1,\n",
        "  ):\n",
        "    super(TransformerDecoderLayer, self).__init__()\n",
        "    dim_k = dim_v = dim_model // num_heads\n",
        "    self.attention_1 = Residual(\n",
        "        MultiHeadAttention(num_heads, dim_model, dim_k, dim_v),\n",
        "        dimension=dim_model,\n",
        "        dropout=dropout\n",
        "    )\n",
        "    self.attention_2 = Residual(\n",
        "        MultiHeadAttention(num_heads, dim_model, dim_k, dim_v),\n",
        "        dimension=dim_model,\n",
        "        dropout=dropout\n",
        "    )\n",
        "    self.feed_forward = Residual(\n",
        "        FeedForward()(dim_model, dim_feedforward),\n",
        "        dimension=dim_model,\n",
        "        dropout=dropout\n",
        "    )\n",
        "  \n",
        "  def forward(self, tgt: Tensor, memory: Tensor) -> Tensor:\n",
        "    tgt = self.attention_1(tgt, tgt, tgt)\n",
        "    tgt = self.attention_2(memory, memory, tgt)\n",
        "    return self.feed_forward(tgt)\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "      num_layers: int = 6,\n",
        "      dim_model: int = 512,\n",
        "      num_heads: int = 8,\n",
        "      dim_feedforward: int = 2048,\n",
        "      dropout: float = 0.1,\n",
        "  ):\n",
        "    super(TransformerDecoder, self).__init__()\n",
        "    self.layers = nn.ModuleList([\n",
        "                                 TransformerDecoderLayer(dim_model, num_heads, dim_feedforward, dropout)\n",
        "                                 for _ in range(num_layers)\n",
        "    ])\n",
        "    self.linear = nn.Linear(dim_model, dim_model)\n",
        "    self.position_encoding = PositionalEncoding()\n",
        "  \n",
        "  def forward(self, tgt: Tensor, memory: Tensor) -> Tensor:\n",
        "    seq_len, dimension = tgt.size(1), tgt.size(2)\n",
        "    tgt += self.position_encoding(seq_len, dimension)\n",
        "    for layer in self.layers:\n",
        "      tgt = layer(tgt, memory)\n",
        "    \n",
        "    return torch.softmax(self.linear(tgt), dim=1)\n",
        "\n",
        "\n",
        "# Transformer\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "  def __init__(\n",
        "      self, \n",
        "      num_encoder_layers: int = 6,\n",
        "      num_decoder_layers: int = 6,\n",
        "      dim_model: int = 512,\n",
        "      num_heads: int = 6,\n",
        "      dim_feedforward: int = 2048,\n",
        "      dropout: float = 0.1,\n",
        "      activation: nn.Module = nn.ReLU(),\n",
        "  ): \n",
        "    super(Transformer, self).__init__()\n",
        "    self.encoder = TransformerEncoder(\n",
        "        num_layers = num_encoder_layers,\n",
        "        dim_model = dim_model,\n",
        "        num_heads = num_heads,\n",
        "        dim_feedforward = dim_feedforward,\n",
        "        dropout = dropout\n",
        "    )\n",
        "    self.decoder = TransformerDecoder(\n",
        "        num_layers = num_decoder_layers,\n",
        "        dim_model = dim_model,\n",
        "        num_heads = num_heads,\n",
        "        dim_feedforward = dim_feedforward,\n",
        "        dropout = dropout\n",
        "    )\n",
        "  \n",
        "  def forward(self, src: Tensor, tgt: Tensor) -> Tensor:\n",
        "    return self.decoder(tgt, self.encoder(src))"
      ],
      "metadata": {
        "id": "IKJBSDFDiEgy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src = torch.rand(64, 16, 512)\n",
        "tgt = torch.rand(64, 16, 512)\n",
        "out = Transformer()(src,tgt)\n",
        "print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7TOu23SkMvf",
        "outputId": "915d8ab5-852f-4378-bdd8-2749a722735b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 16, 512])\n"
          ]
        }
      ]
    }
  ]
}